@article{LASSO1996,
abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
author = {Tibshirani, Robert},
doi = {10.1111/j.2517-6161.1996.tb02080.x},
file = {:C\:/Users/YuQiFeng/Desktop/变量选择-冯裕祺/paper/1995lasso.pdf:pdf},
issn = {0035-9246},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
keywords = {quadratic programming,regression,shrinkage,subset selection},
number = {1},
pages = {267--288},
title = {{Regression Shrinkage and Selection Via the Lasso}},
volume = {58},
year = {1996}
}
@article{SCAD2001,
abstract = {Variable selection is fundamental to high-dimensional statistical modeling, including nonparametric regression. Many approaches in use are stepwise selection procedures, which can be computationally expensive and ignore stochastic errors in the variable selection process. In this article, penalized likelihood approaches are proposed to handle these kinds of problems. The proposed methods select variables and estimate coefficients simultaneously. Hence they enable us to construct confidence intervals for estimated parameters. The proposed approaches are distinguished from others in that the penalty functions are symmetric, nonconcave on (0, ∞), and have singularities at the origin to produce sparse solutions. Furthermore, the penalty functions should be bounded by a constant to reduce bias and satisfy certain conditions to yield continuous solutions. A new algorithm is proposed for optimizing penalized likelihood functions. The proposed ideas are widely applicable. They are readily applied to a variety of parametric models such as generalized linear models and robust regression models. They can also be applied easily to nonparametric modeling by using wavelets and splines. Rates of convergence of the proposed penalized likelihood estimators are established. Furthermore, with proper choice of regularization parameters, we show that the proposed estimators perform as well as the oracle procedure in variable selection; namely, they work as well as if the correct submodel were known. Our simulation shows that the newly proposed methods compare favorably with other variable selection techniques. Furthermore, the standard error formulas are tested to be accurate enough for practical applications. {\textcopyright} 2001, Taylor & Francis Group, LLC. All rights reserved.},
author = {Fan, Jianqing and Li, Runze},
doi = {10.1198/016214501753382273},
file = {:C\:/Users/YuQiFeng/Desktop/变量选择-冯裕祺/paper/2001FanLi.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Hard thresholding,LASSO,Nonnegative garrote,Oracle estimator,Penalized likelihood,SCAD,Soft thresholding},
number = {456},
pages = {1348--1360},
title = {{Variable selection via nonconcave penalized likelihood and its oracle properties}},
volume = {96},
year = {2001}
}
@article{Zou2005,
author = {Zou, Hui and Hastie, Trevor},
doi = {10.1111/j.1467-9868.2005.00527.x},
file = {:C\:/Users/YuQiFeng/Desktop/变量选择-冯裕祺/paper/2005elasticnet.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {grouping effect,lars algorithm,lasso,p,penalization},
number = {5},
pages = {768},
title = {{Erratum: Regularization and variable selection via the elastic net (Journal of the Royal Statistical Society. Series B: Statistical Methodology (2005) 67 (301-320))}},
volume = {67},
year = {2005}
}
@article{Meinshausen2006,
abstract = {The pattern of zero entries in the inverse covariance matrix of a multivariate normal distribution corresponds to conditional independence restrictions between variables. Covariance selection aims at estimating those structural zeros from data. We show that neighborhood selection with the Lasso is a computationally attractive alternative to standard covariance selection for sparse high-dimensional graphs. Neighborhood selection estimates the conditional independence restrictions separately for each node in the graph and is hence equivalent to variable selection for Gaussian linear models. We show that the proposed neighborhood selection scheme is consistent for sparse high-dimensional graphs. Consistency hinges on the choice of the penalty parameter. The oracle value for optimal prediction does not lead to a consistent neighborhood estimate. Controlling instead the probability of falsely joining some distinct connectivity components of the graph, consistent estimation for sparse graphs is achieved (with exponential rates), even when the number of variables grows as the number of observations raised to an arbitrary power. {\textcopyright} Institute of Mathematical Statistics, 2006.},
archivePrefix = {arXiv},
arxivId = {math/0608017},
author = {Meinshausen, Nicolai and B{\"{u}}hlmann, Peter},
doi = {10.1214/009053606000000281},
eprint = {0608017},
file = {:C\:/Users/YuQiFeng/Desktop/变量选择-冯裕祺/paper/2006High-dimensional graphs and variable selection with the Lasso.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Covariance selection,Gaussian graphical models,Linear regression,Penalized regression},
number = {3},
pages = {1436--1462},
primaryClass = {math},
title = {{High-dimensional graphs and variable selection with the Lasso}},
volume = {34},
year = {2006}
}
@article{Zou2006,
abstract = {The lasso is a popular technique for simultaneous estimation and variable selection. Lasso variable selection has been shown to be consistent under certain conditions. In this work we derive a necessary condition for the lasso variable selection to be consistent. Consequently, there exist certain scenarios where the lasso is inconsistent for variable selection. We then propose a new version of the lasso, called the adaptive lasso, where adaptive weights are used for penalising different coefficients in the ℓ1 penalty. We show that the adaptive lasso enjoys the oracle properties; namely, it performs as well us if the true underlying model were given in advance. Similar to the lasso, the adaptive lasso is shown to be near-minimax optimal. Furthermore, the adaptive lasso can be solved by the same efficient algorithm for solving the lasso. We also discuss the extension of the adaptive lasso in generalized linear models and show that the oracle properties still hold under mild regularity conditions. As a byproduct of our theory, the nonnegalive garotte is shown to be consistent for variable selection. {\textcopyright} 2006 American Statistical Association.},
author = {Zou, Hui},
doi = {10.1198/016214506000000735},
file = {:C\:/Users/YuQiFeng/Desktop/变量选择-冯裕祺/paper/2006The Adaptive Lasso and Its Oracle Properties.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Asymptotic normality,Lasso,Minimax,Oracle inequality,Oracle procedure,Variable selection},
number = {476},
pages = {1418--1429},
title = {{The adaptive lasso and its oracle properties}},
volume = {101},
year = {2006}
}
@article{Candes2007,
abstract = {In many important statistical applications, the number of variables or parameters p is much larger than the number of observations n. Suppose then that we have observations y = X$\beta$ + z, where $\beta$ ∈ Rp is a parameter vector of interest, X is a data matrix with possibly far fewer rows than columns, n ≪ p, and the zi's are i.i.d. N(0, $\sigma$2). Is it possible to estimate $\beta$ reliably based on the noisy data y? To estimate $\beta$, we introduce a new estimator - we call it the Dantzig selector-which is a solution to the ℓ1-regularization problem equation persented where r is the residual vector y - X $\beta$̃ and t is a positive scalar. We show that if X obeys a uniform uncertainty principle (with unit-normed columns) and if the true parameter vector $\beta$ is sufficiently sparse (which here roughly guarantees that the model is identifiable), then with very large probability, equation presented Our results are nonasymptotic and we give values for the constant C. Even though n may be much smaller than p, our estimator achieves a loss within a logarithmic factor of the ideal mean squared error one would achieve with an oracle which would supply perfect information about which coordinates are nonzero, and which were above the noise level. In multivariate regression and from a model selection viewpoint, our result says that it is possible nearly to select the best subset of variables by solving a very simple convex program, which, in fact, can easily be recast as a convenient linear program (LP). {\textcopyright} Institute of Mathematical Statistics, 2007.},
archivePrefix = {arXiv},
arxivId = {math/0506081},
author = {Candes, Emmanuel and Tao, Terence},
doi = {10.1214/009053606000001523},
eprint = {0506081},
file = {:C\:/Users/YuQiFeng/Desktop/变量选择-冯裕祺/paper/2007danzig selector.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Geometry in high dimensions,Ideal estimation,Linear programming,Model selection,Oracle inequalities,Random matrices,Restricted orthonormality,Sparse solutions to underdetermined systems,Statistical linear model,ℓ1- minimization},
number = {6},
pages = {2313--2351},
primaryClass = {math},
title = {{The Dantzig selector: Statistical estimation when p is much larger than n}},
volume = {35},
year = {2007}
}
@article{Fan2008,
abstract = {Summary. Variable selection plays an important role in high dimensional statistical modelling which nowadays appears in many areas and is key to various scientific discoveries. For problems of large scale or dimensionality p, accuracy of estimation and computational cost are two top concerns. Recently, Candes and Tao have proposed the Dantzig selector using L1- regularization and showed that it achieves the ideal risk up to a logarithmic factor log (p). Their innovative procedure and remarkable result are challenged when the dimensionality is ultrahigh as the factor log (p) can be large and their uniform uncertainty principle can fail. Motivated by these concerns, we introduce the concept of sure screening and propose a sure screening method that is based on correlation learning, called sure independence screening, to reduce dimensionality from high to a moderate scale that is below the sample size. In a fairly general asymptotic framework, correlation learning is shown to have the sure screening property for even exponentially growing dimensionality. As a methodological extension, iterative sure independence screening is also proposed to enhance its finite sample performance. With dimension reduced accurately from high to below sample size, variable selection can be improved on both speed and accuracy, and can then be accomplished by a well-developed method such as smoothly clipped absolute deviation, the Dantzig selector, lasso or adaptive lasso. The connections between these penalized least squares methods are also elucidated. {\textcopyright} 2008 Royal Statistical Society.},
archivePrefix = {arXiv},
arxivId = {math/0612857},
author = {Fan, Jianqing and Lv, Jinchi},
doi = {10.1111/j.1467-9868.2008.00674.x},
eprint = {0612857},
file = {:C\:/Users/YuQiFeng/Desktop/变量选择-冯裕祺/paper/2008Sure independence screening for ultrahighdimensional feature space.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Adaptive lasso,Dantzig selector,Dimensionality reduction,Lasso,Oracle estimator,Smoothly clipped absolute deviation,Sure independence screening,Sure screening,Variable selection},
number = {5},
pages = {849--911},
primaryClass = {math},
title = {{Sure independence screening for ultrahigh dimensional feature space}},
volume = {70},
year = {2008}
}
@article{Friedman2010,
abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include l1 (the lasso), l2 (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
doi = {10.18637/jss.v033.i01},
file = {:C\:/Users/YuQiFeng/Desktop/变量选择-冯裕祺/paper/2010glmnet.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Coordinate-descent,Elastic net,L1 penalty,Lasso,Logistic regression,Regularization path},
number = {1},
pages = {1--22},
pmid = {20808728},
title = {{Regularization paths for generalized linear models via coordinate descent}},
volume = {33},
year = {2010}
}
@book{Zhang2010,
abstract = {We propose MC+, a fast, continuous, nearly unbiased and accurate method of penalized variable selection in high-dimensional linear regression. The LASSO is fast and continuous, but biased. The bias of the LASSO may prevent consistent variable selection. Subset selection is unbiased but computationally costly. The MC+ has two elements: a minimax concave penalty (MCP) and a penalized linear unbiased selection (PLUS) algorithm. The MCP provides the convexity of the penalized loss in sparse regions to the greatest extent given certain thresholds for variable selection and unbiasedness. The PLUS computes multiple exact local minimizers of a possibly nonconvex penalized loss function in a certain main branch of the graph of critical points of the penalized loss. Its output is a continuous piecewise linear path encompassing from the origin for infinite penalty to a least squares solution for zero penalty.We prove that at a universal penalty level, the MC+ has high probability of matching the signs of the unknowns, and thus correct selection, without assuming the strong irrepresentable condition required by the LASSO. This selection consistency applies to the case of p ≫ n, and is proved to hold for exactly theMC+ solution among possibly many local minimizers. We prove that the MC+ attains certain minimax convergence rates in probability for the estimation of regression coefficients in ℓr balls. We use the SURE method to derive degrees of freedom and Cp-type risk estimates for general penalized LSE, including the LASSO and MC+ estimators, and prove their unbiasedness. Based on the estimated degrees of freedom, we propose an estimator of the noise level for proper choice of the penalty level. For full rank designs and general sub-quadratic penalties, we provide necessary and sufficient conditions for the continuity of the penalized LSE. Simulation results overwhelmingly support our claim of superior variable selection properties and demonstrate the computational efficiency of the proposed method. {\textcopyright} 2010 Institute of Mathematical Statistics.},
author = {Zhang, Cun Hui},
booktitle = {Annals of Statistics},
doi = {10.1214/09-AOS729},
file = {:C\:/Users/YuQiFeng/Desktop/变量选择-冯裕祺/paper/2010NEARLY UNBIASED VARIABLE SELECTION UNDERMINIMAX CONCAVE PENALTY.pdf:pdf},
isbn = {9040210063},
issn = {00905364},
keywords = {Correct selection,Degrees of freedom,Least squares,Mean squared error,Minimax,Model selection,Nonconvex minimization,Penalized estimation,Risk estimation,Selection consistency,Sign consistency,Unbiasedness,Variable selection},
number = {2},
pages = {894--942},
title = {{Nearly unbiased variable selection under minimax concave penalty}},
volume = {38},
year = {2010}
}
@article{Fan2010,
abstract = {Ultrahigh-dimensional variable selection plays an increasingly important role in contemporary scientific discoveries and statistical research. Among others, Fan and Lv [J. R. Stat. Soc. Ser. B Stat. Methodol. 70 (2008) 849-911] propose an independent screening framework by ranking the marginal correlations. They showed that the correlation ranking procedure possesses a sure independence screening property within the context of the linear model with Gaussian covariates and responses. In this paper, we propose a more general version of the independent learning with ranking the maximum marginal likelihood estimates or themaximum marginal likelihood itself in generalized linear models. We show that the proposed methods, with Fan and Lv [J. R. Stat. Soc. Ser. B Stat. Methodol. 70 (2008) 849-911] as a very special case, also possess the sure screening property with vanishing false selection rate. The conditions under which the independence learning possesses a sure screening is surprisingly simple. This justifies the applicability of such a simple method in a wide spectrum. We quantify explicitly the extent to which the dimensionality can be reduced by independence screening, which depends on the interactions of the covariance matrix of covariates and true parameters. Simulation studies are used to illustrate the utility of the proposed approaches. In addition, we establish an exponential inequality for the quasi-maximum likelihood estimator which is useful for high-dimensional statistical learning. {\textcopyright} Institute of Mathematical Statistics, 2010.},
author = {Fan, Jianqing and Song, Rui},
doi = {10.1214/10-AOS798},
file = {:C\:/Users/YuQiFeng/Desktop/变量选择-冯裕祺/paper/2010SURE INDEPENDENCE SCREENING IN GENERALIZED LINEARMODELS WITH NP-DIMENSIONALITY.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Generalized linear models,Independent learning,Sure independent screening,Variable selection},
number = {6},
pages = {3567--3604},
title = {{Sure independence screening in generalized linear models with NP-dimensionality}},
volume = {38},
year = {2010}
}
@article{Li2012,
abstract = {This article is concerned with screening features in ultrahigh-dimensional data analysis, which has become increasingly important in diverse scientific fields. We develop a sure independence screening procedure based on the distance correlation (DC-SIS). The DC-SIS can be implemented as easily as the sure independence screening (SIS) procedure based on the Pearson correlation proposed by Fan and Lv. However, the DC-SIS can significantly improve the SIS. Fan and Lv established the sure screening property for the SIS based on linear models, but the sure screening property is valid for the DC-SIS under more general settings, including linear models. Furthermore, the implementation of the DC-SIS does not require model specification (e.g., linear model or generalized linear model) for responses or predictors. This is a very appealing property in ultrahigh-dimensional data analysis. Moreover, the DC-SIS can be used directly to screen grouped predictor variables and multivariate response variables. We establish the sure screening property for the DC-SIS, and conduct simulations to examine its finite sample performance. A numerical comparison indicates that the DC-SIS performs much better than the SIS in various models. We also illustrate the DC-SIS through a real-data example. {\textcopyright} 2012 American Statistical Association.},
archivePrefix = {arXiv},
arxivId = {1205.4701},
author = {Li, Runze and Zhong, Wei and Zhu, Liping},
doi = {10.1080/01621459.2012.695654},
eprint = {1205.4701},
file = {:C\:/Users/YuQiFeng/Desktop/变量选择-冯裕祺/paper/2012Feature_Screening_via_Distance_Correlation_Learnin.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Sure independence screening,Sure screening property,Ultrahigh dimensionality,Variable selection},
number = {499},
pages = {1129--1139},
title = {{Feature screening via distance correlation learning}},
volume = {107},
year = {2012}
}
@article{Fan2020,
author = {Fan, Jianqing and Li, Runze},
doi = {10.1080/00401706.2020.1801256},
file = {:C\:/Users/YuQiFeng/Desktop/变量选择-冯裕祺/paper/2020comment feature screening and variable selection.pdf:pdf},
issn = {15372723},
journal = {Technometrics},
keywords = {Penalized least squares,penalized likelihood,matri},
number = {4},
pages = {434--437},
publisher = {Taylor & Francis},
title = {{Comment: Feature Screening and Variable Selection via Iterative Ridge Regression}},
url = {http://dx.doi.org/10.1080/00401706.2020.1801256},
volume = {62},
year = {2020}
}
@article{Fan2020a,
abstract = {Best subset selection (BSS) is fundamental in statistics and machine learning. Despite the intensive studies of it, the fundamental question of when BSS is truly the “best”, namely yielding the oracle estimator, remains partially answered. In this paper, we address this important issue by giving a weak sufficient condition and a strong necessary condition for BSS to exactly recover the true model. We also give a weak sufficient condition for BSS to achieve the sure screening property. On the optimization aspect, we find that the exact combinatorial minimizer for BSS is unnecessary: all the established statistical properties for the best subset carry over to any sparse model whose residual sum of squares is close enough to that of the best subset. In particular, we show that an iterative hard thresholding (IHT) algorithm can find a sparse subset with the sure screening property within logarithmic steps; another round of BSS within this set can recover the true model. The simulation studies and real data examples show that IHT yields lower false discovery rates and higher true positive rates than the competing approaches including LASSO, SCAD and SIS.},
archivePrefix = {arXiv},
arxivId = {2007.01478},
author = {Fan, Jianqing and Zhu, Ziwei},
eprint = {2007.01478},
file = {:C\:/Users/YuQiFeng/Desktop/变量选择-冯裕祺/paper/2020When is best subset selection the best.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Best Subset Selection,High-Dimensional Variable Selection,Iterative Hard Thresholding,Model Consistency,True Positive Rate},
title = {{When is best subset selection the “best”?}},
year = {2020}
}
@article{Zhu2021,
abstract = {Best-subset selection aims to find a small subset of predictors, so that the resulting linear model is expected to have the most desirable prediction accuracy. It is not only important and imperative in regression analysis but also has far-reaching applications in every facet of research, including computer science and medicine. We introduce a polynomial algorithm, which, under mild conditions, solves the problem. This algorithm exploits the idea of sequencing and splicing to reach a stable solution in finite steps when the sparsity level of the model is fixed but unknown. We define an information criterion that helps the algorithm select the true sparsity level with a high probability. We show that when the algorithm produces a stable optimal solution, that solution is the oracle estimator of the true parameters with probability one. We also demonstrate the power of the algorithm in several numerical studies.},
author = {Zhu, Junxian and Wen, Canhong and Zhu, Jin and Zhang, Heping and Wang, Xueqin},
doi = {10.1073/PNAS.2014241117},
file = {:C\:/Users/YuQiFeng/Desktop/变量选择-冯裕祺/paper/2020A polynomial algorithm for best-subset.pdf:pdf},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Best-subset selection,High dimensional,Splicing},
number = {52},
pages = {33117--33123},
pmid = {33328272},
title = {{A polynomial algorithm for best-subset selection problem}},
volume = {117},
year = {2021}
}
@article{Zhu2021a,
abstract = {In deep learning tasks, the update step size determined by the learning rate at each iteration plays a critical role in gradient-based optimization. However, determining the appropriate learning rate in practice typically relies on subjective judgment. In this work, we propose a novel optimization method based on local quadratic approximation (LQA). In each update step, we locally approximate the loss function along the gradient direction by using a standard quadratic function of the learning rate. Subsequently, we propose an approximation step to obtain a nearly optimal learning rate in a computationally efficient manner. The proposed LQA method has three important features. First, the learning rate is automatically determined in each update step. Second, it is dynamically adjusted according to the current loss function value and parameter estimates. Third, with the gradient direction fixed, the proposed method attains a nearly maximum reduction in the loss function. Extensive experiments were conducted to prove the effectiveness of the proposed LQA method.},
archivePrefix = {arXiv},
arxivId = {2004.03260},
author = {Zhu, Yingqiu and Huang, Danyang and Gao, Yuan and Wu, Rui and Chen, Yu and Zhang, Bo and Wang, Hansheng},
doi = {10.1016/j.neunet.2021.03.025},
eprint = {2004.03260},
file = {:C\:/Users/YuQiFeng/Desktop/变量选择-冯裕祺/paper/2020Automatic, Dynamic, and Nearly Optimal Learning Rate Specification by Local Quadratic Approximation.pdf:pdf},
issn = {18792782},
journal = {Neural Networks},
keywords = {Gradient descent,Gradient-based optimization,Learning rate,Local quadratic approximation,Machine learning,Neural networks},
pages = {11--29},
title = {{Automatic, dynamic, and nearly optimal learning rate specification via local quadratic approximation}},
volume = {141},
year = {2021}
}
@article{Huang2021,
author = {Huang, Danyang and Zhu, Xuening and Li, Runze and Wang, Hansheng},
doi = {10.5705/ss.202018.0400},
file = {:C\:/Users/YuQiFeng/Desktop/变量选择-冯裕祺/paper/2021Feature Screening for Network Autoregression Model.pdf:pdf},
issn = {10170405},
journal = {Statistica Sinica},
title = {{Feature Screening for Network Autoregression Model}},
year = {2021}
}
@article{leastangle2004,
	title="Least angle regression",
	author="Bradley {Efron} and Trevor {Hastie} and Iain {Johnstone} and Robert {Tibshirani} and Hemant {Ishwaran} and Keith {Knight} and Jean Michel {Loubes} and Pascal {Massart} and David {Madigan} and Greg {Ridgeway} and Saharon {Rosset} and J. I. {Zhu} and Robert A. {Stine} and Berwin A. {Turlach} and Sanford {Weisberg}",
	journal="Annals of Statistics",
	volume="32",
	number="2",
	pages="407--499",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2063978378",
	year="2004"
}
@article{Qi2019,
abstract = {The standard support vector machine (SVM) models are widely used in various fields, but we show that they are not rationally defined from the perspective of geometric point, which is likely to degrade the models' performances theoretically, especially under the high-dimensional cases. In this paper, we consider a composite penalty and propose an elastic net support vector machine (ENSVM). Unlike the doubly regularized support vector machine (DrSVM, Wang et al. (2006)), we impose the penalty to the slack variables rather than the normal vectors of the hyperplane. Then, we prove that ENSVM is more rationally defined than standard SVM and DrSVM (in section 3.2.1). Moreover, the ENSVM demonstrates a more stable and high-dimension nature inherently, while the simulation results cogently support these merits. Besides, we also combine fused weights with ENSVM and propose an adaptive weighted elastic net support vector machine (AWENSVM), to make the primal model more adaptive and robust to the imbalanced data. Compared with the other popular SVMs, the AWENSVM model proposed in this paper performs better obviously.},
author = {Qi, Kai and Yang, Hu and Hu, Qingyu and Yang, Dongjun},
doi = {10.1016/j.knosys.2019.104933},
file = {:C\:/Users/YuQiFeng/Desktop/变量选择-冯裕祺/paper/2019a new adaptive weighted imbalanced data classifier via improved support vector machines with high-dimension nature.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Adaptive weights,High dimension,Imbalanced data,Support vector machine},
pages = {104933},
publisher = {Elsevier B.V.},
title = {{A new adaptive weighted imbalanced data classifier via improved support vector machines with high-dimension nature}},
url = {https://doi.org/10.1016/j.knosys.2019.104933},
volume = {185},
year = {2019}
}
@article{Wen2020,
abstract = {We introduce a new R package, BeSS, for solving the best subset selection problem in linear, logistic and Cox's proportional hazard (CoxPH) models. It utilizes a highly efficient active set algorithm based on primal and dual variables, and supports sequential and golden search strategies for best subset selection. We provide a C++ implementation of the algorithm using an Rcpp interface. We demonstrate through numerical experiments based on enormous simulation and real datasets that the new BeSS package has competitive performance compared to other R packages for best subset selection purposes.},
author = {Wen, Canhong and Zhang, Aijun and Wang, Xueqin and Quan, Shijie},
doi = {10.18637/jss.v094.i04},
file = {:C\:/Users/YuQiFeng/Desktop/machineleaRING/paper/2020BeSS.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Best subset selection,C++,Model selection,Primal dual active set,R,Rcpp,Variable selection},
number = {4},
pages = {1--24},
title = {{Bess: An R package for best subset selection in linear, logistic and cox proportional hazards models}},
volume = {94},
year = {2020}
}
@article{Zhu2021,
abstract = {Best-subset selection aims to find a small subset of predictors, so that the resulting linear model is expected to have the most desirable prediction accuracy. It is not only important and imperative in regression analysis but also has far-reaching applications in every facet of research, including computer science and medicine. We introduce a polynomial algorithm, which, under mild conditions, solves the problem. This algorithm exploits the idea of sequencing and splicing to reach a stable solution in finite steps when the sparsity level of the model is fixed but unknown. We define an information criterion that helps the algorithm select the true sparsity level with a high probability. We show that when the algorithm produces a stable optimal solution, that solution is the oracle estimator of the true parameters with probability one. We also demonstrate the power of the algorithm in several numerical studies.},
author = {Zhu, Junxian and Wen, Canhong and Zhu, Jin and Zhang, Heping and Wang, Xueqin},
doi = {10.1073/PNAS.2014241117},
file = {:C\:/Users/YuQiFeng/Desktop/变量选择-冯裕祺/paper/2020A polynomial algorithm for best-subset.pdf:pdf},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Best-subset selection,High dimensional,Splicing},
number = {52},
pages = {33117--33123},
pmid = {33328272},
title = {{A polynomial algorithm for best-subset selection problem}},
volume = {117},
year = {2021}
}
@article{hocking1967selection,
  title={Selection of the best subset in regression analysis},
  author={Hocking, Ronald R and Leslie, RN},
  journal={Technometrics},
  volume={9},
  number={4},
  pages={531--540},
  year={1967},
  publisher={Taylor \& Francis Group}
}
@article{Zhu2021,
abstract = {Best-subset selection aims to find a small subset of predictors, so that the resulting linear model is expected to have the most desirable prediction accuracy. It is not only important and imperative in regression analysis but also has far-reaching applications in every facet of research, including computer science and medicine. We introduce a polynomial algorithm, which, under mild conditions, solves the problem. This algorithm exploits the idea of sequencing and splicing to reach a stable solution in finite steps when the sparsity level of the model is fixed but unknown. We define an information criterion that helps the algorithm select the true sparsity level with a high probability. We show that when the algorithm produces a stable optimal solution, that solution is the oracle estimator of the true parameters with probability one. We also demonstrate the power of the algorithm in several numerical studies.},
author = {Zhu, Junxian and Wen, Canhong and Zhu, Jin and Zhang, Heping and Wang, Xueqin},
doi = {10.1073/PNAS.2014241117},
file = {:C\:/Users/YuQiFeng/Desktop/变量选择-冯裕祺/paper/2020A polynomial algorithm for best-subset.pdf:pdf},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Best-subset selection,High dimensional,Splicing},
number = {52},
pages = {33117--33123},
pmid = {33328272},
title = {{A polynomial algorithm for best-subset selection problem}},
volume = {117},
year = {2021}
}
@inproceedings{li2008hybrid,
  title={A hybrid re-sampling method for SVM learning from imbalanced data sets},
  author={Li, Peng and Qiao, Pei-Li and Liu, Yuan-Chao},
  booktitle={2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery},
  volume={2},
  pages={65--69},
  year={2008},
  organization={IEEE}
}
@article{tang2008svms,
  title={SVMs modeling for highly imbalanced classification},
  author={Tang, Yuchun and Zhang, Yan-Qing and Chawla, Nitesh V and Krasser, Sven},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  volume={39},
  number={1},
  pages={281--288},
  year={2008},
  publisher={IEEE}
}
@inproceedings{zou2008svm,
  title={SVM learning from imbalanced data by GA sampling for protein domain prediction},
  author={Zou, Shuxue and Huang, Yanxin and Wang, Yan and Wang, Jianxin and Zhou, Chunguang},
  booktitle={2008 the 9th international conference for young computer scientists},
  pages={982--987},
  year={2008},
  organization={IEEE}
}
@article{hwang2011new,
  title={A new weighted approach to imbalanced data classification problem via support vector machine with quadratic cost function},
  author={Hwang, Jae Pil and Park, Seongkeun and Kim, Euntai},
  journal={Expert Systems with Applications},
  volume={38},
  number={7},
  pages={8580--8585},
  year={2011},
  publisher={Elsevier}
}
